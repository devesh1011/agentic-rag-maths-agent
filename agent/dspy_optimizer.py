import dspy
import os
import json
from dotenv import load_dotenv
from dspy_model import MathRAG


load_dotenv()


gemini_llm = dspy.LM("gemini/gemini-2.5-flash", api_key=os.getenv("GOOGLE_API_KEY"))
dspy.settings.configure(lm=gemini_llm)


class SolutionQualityMetric(dspy.Signature):
    """Evaluate the quality of a generated math solution based on the ground truth."""

    ground_truth_solution = dspy.InputField(desc="The ideal, correct solution.")
    predicted_solution = dspy.InputField(
        desc="The solution generated by the language model."
    )
    quality_score = dspy.OutputField(
        desc="A score from 1 to 5, where 5 is excellent and correct."
    )


def llm_judge(gold, pred, trace=None):
    """metric that uses an LLM to score the generated solution."""

    predicted_solution = pred.solution
    ground_truth_solution = gold.solution

    # Make prediction
    scorer = dspy.Predict(SolutionQualityMetric)
    result = scorer(
        ground_truth_solution=ground_truth_solution,
        predicted_solution=predicted_solution,
    )

    try:
        score_str = "".join(filter(str.isdigit, result.quality_score))
        score = int(score_str) if score_str else 0
    except (ValueError, TypeError):
        score = 0

    print(f"Judged Quality Score: {score}")
    return score >= 4


with open("agent/data/feedback.json", "r") as f:
    feedback_data = json.load(f)

examples = []
for ex_data in feedback_data.values():
    is_correction = len(ex_data["human_feedback"]) > 20
    correct_solution = (
        ex_data["human_feedback"] if is_correction else ex_data["proposed_answer"]
    )

    example = dspy.Example(
        question=ex_data["question"],
        context=ex_data["context"],
        solution=correct_solution,
    ).with_inputs("question", "context")
    examples.append(example)

# Split the data
trainset = examples[:2]
devset = examples[2:]


# initialize dspy BootstrapFewShot optimizer
optimizer = dspy.BootstrapFewShot(metric=llm_judge, max_bootstrapped_demos=2)
optimized_agent = optimizer.compile(MathRAG(), trainset=trainset)


# evaluate the math agent on the devset
evaluate = dspy.Evaluate(
    devset=devset, num_threads=1, display_progress=True, display_table=5
)
evaluate(optimized_agent, metric=llm_judge)

# save the prompt
optimized_agent.save("agent/math_rag_optimized.json")
